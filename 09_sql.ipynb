{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "250df6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b92067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@7b3cd46\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@7b3cd46"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", \"/user/itv001389/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    master(\"yarn\").\n",
    "    appName(\"Getting Started - Spark with Scala\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf453c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:27: error: not found: value ls\n       !ls\n        ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73fc8e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use raj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d82f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------+-----------+\n",
      "|database |tableName                       |isTemporary|\n",
      "+---------+--------------------------------+-----------+\n",
      "|itv001389|bucketed_user                   |false      |\n",
      "|itv001389|new_test                        |false      |\n",
      "|itv001389|order_items_bucket              |false      |\n",
      "|itv001389|order_item_sra                  |false      |\n",
      "|itv001389|order_par                       |false      |\n",
      "|itv001389|order_rev                       |false      |\n",
      "|itv001389|order_rev1                      |false      |\n",
      "|itv001389|orders_header_skip              |false      |\n",
      "|itv001389|orders_header_skip_immu         |false      |\n",
      "|itv001389|orders_orc_snappy               |false      |\n",
      "|itv001389|orders_par_bucket               |false      |\n",
      "|itv001389|orders_partition_order_date     |false      |\n",
      "|itv001389|orders_partition_order_date_date|false      |\n",
      "|itv001389|orders_partition_order_status   |false      |\n",
      "|itv001389|order_sra                       |false      |\n",
      "|itv001389|orders_raj                      |false      |\n",
      "|itv001389|orders_raj_external             |false      |\n",
      "|itv001389|orders_raj_nopath               |false      |\n",
      "|itv001389|orders_raj_nopath_text_script   |false      |\n",
      "|itv001389|orders_raj_nopath_text_script1  |false      |\n",
      "+---------+--------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6392e9a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException",
     "evalue": "Table or view 'joins_sra' already exists in database 'raj';",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: Table or view 'joins_sra' already exists in database 'raj';",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:319)",
      "  at org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:130)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)",
      "  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)",
      "  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)",
      "  ... 42 elided"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create table raj.joins_sra(Id int,Name string) row format delimited fields terminated by ',' stored as TEXTFILE location 'hdfs://m01.itversity.com:9000/user/itv001389/warehouse/raj.db/Join1' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb53c4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|Id |Name|\n",
      "+---+----+\n",
      "|1  |Raj |\n",
      "|2  |Sai |\n",
      "|3  |Hari|\n",
      "|5  |Chai|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from raj.joins_sra\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb834ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"alter table raj.joins_sra rename to raj.join1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da9be413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|Id |Name|\n",
      "+---+----+\n",
      "|1  |Raj |\n",
      "|2  |Sai |\n",
      "|3  |Hari|\n",
      "|5  |Chai|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from raj.join1\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7217c658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table raj.join2(Id int,Place String) row format delimited fields terminated by ',' stored as textfile location 'hdfs://m01.itversity.com:9000/user/itv001389/warehouse/raj.db/Join2' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03e72c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|Id |Place    |\n",
      "+---+---------+\n",
      "|1  |Bangalore|\n",
      "|2  |Chennai  |\n",
      "|4  |Hyderabad|\n",
      "|6  |Bangalore|\n",
      "|7  |Chennai  |\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from raj.join2\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88568660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+\n",
      "|Id |Name|Place    |\n",
      "+---+----+---------+\n",
      "|1  |Raj |Bangalore|\n",
      "|2  |Sai |Chennai  |\n",
      "+---+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.Id,a.Name,b.Place from raj.join1 a join raj.join2 b on a.Id=b.Id\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3233d674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+---------+\n",
      "|Id |Name|Id  |Place    |\n",
      "+---+----+----+---------+\n",
      "|1  |Raj |1   |Bangalore|\n",
      "|2  |Sai |2   |Chennai  |\n",
      "|3  |Hari|null|null     |\n",
      "|5  |Chai|null|null     |\n",
      "+---+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.Id,a.Name,b.Id,b.Place from raj.join1 a left outer join raj.join2 b on a.Id=b.Id\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e54c61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+---------+\n",
      "|Id  |Name|Id |Place    |\n",
      "+----+----+---+---------+\n",
      "|1   |Raj |1  |Bangalore|\n",
      "|2   |Sai |2  |Chennai  |\n",
      "|null|null|4  |Hyderabad|\n",
      "|null|null|6  |Bangalore|\n",
      "|null|null|7  |Chennai  |\n",
      "+----+----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.Id,a.Name,b.Id,b.Place from raj.join1 a right outer join raj.join2 b on a.Id=b.Id\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85e237ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---------+\n",
      "|Id  |Name|Id  |Place    |\n",
      "+----+----+----+---------+\n",
      "|1   |Raj |1   |Bangalore|\n",
      "|null|null|6   |Bangalore|\n",
      "|3   |Hari|null|null     |\n",
      "|5   |Chai|null|null     |\n",
      "|null|null|4   |Hyderabad|\n",
      "|null|null|7   |Chennai  |\n",
      "|2   |Sai |2   |Chennai  |\n",
      "+----+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.Id,a.Name,b.Id,b.Place from raj.join1 a full outer join raj.join2 b on a.Id=b.Id\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbcda5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+---------+\n",
      "|Id |Name|Id |Place    |\n",
      "+---+----+---+---------+\n",
      "|1  |Raj |1  |Bangalore|\n",
      "|2  |Sai |2  |Chennai  |\n",
      "+---+----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.Id,a.Name,b.Id,b.Place from raj.join1 a, raj.join2 b where a.Id=b.Id\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f361f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2 - Scala",
   "language": "scala",
   "name": "spark_2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
